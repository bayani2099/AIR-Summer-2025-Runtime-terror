{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MfnQvGfvtH1w"
   },
   "source": [
    "# Getting started\n",
    "\n",
    "### CLEF 2025 - CheckThat! Lab  - Task 4 Scientific Web Discourse - Subtask 4b (Scientific Claim Source Retrieval)\n",
    "\n",
    "This notebook enables participants of subtask 4b to quickly get started. It includes the following:\n",
    "- Code to upload data, including:\n",
    "    - code to upload the collection set (CORD-19 academic papers' metadata)\n",
    "    - code to upload the query set (tweets with implicit references to CORD-19 papers)\n",
    "- Code to run a baseline retrieval model (BM25)\n",
    "- Code to evaluate the baseline model\n",
    "\n",
    "Participants are free to use this notebook and add their own models for the competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpDCfBMouNAL"
   },
   "source": [
    "# 1) Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 403,
     "status": "ok",
     "timestamp": 1742975967136,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "rQPqDKP_QHFM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pickle\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u8N7h9BhQI5m"
   },
   "source": [
    "## 1.a) Import the collection set\n",
    "The collection set contains metadata of CORD-19 academic papers.\n",
    "\n",
    "The preprocessed and filtered CORD-19 dataset is available on the Gitlab repository here: https://gitlab.com/checkthat_lab/clef2025-checkthat-lab/-/tree/main/task4/subtask_4b\n",
    "\n",
    "Participants should first download the file then upload it on the Google Colab session with the following steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1742975971100,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "2GQI4HcKR6hS"
   },
   "outputs": [],
   "source": [
    "# 1) Download the collection set from the Gitlab repository: https://gitlab.com/checkthat_lab/clef2025-checkthat-lab/-/tree/main/task4/subtask_4b\n",
    "# 2) Drag and drop the downloaded file to the \"Files\" section (left vertical menu on Colab)\n",
    "# 3) Modify the path to your local file path\n",
    "PATH_COLLECTION_DATA = 'subtask4b_collection_data.pkl' #MODIFY PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1742975975524,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "SYBB3UYbMwTA"
   },
   "outputs": [],
   "source": [
    "df_collection = pd.read_pickle(PATH_COLLECTION_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1742975976305,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "4v3lygNOQQSn",
    "outputId": "ee5b9abd-f889-4a4e-ce11-32d2691433cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 7718 entries, 162 to 1056448\n",
      "Data columns (total 17 columns):\n",
      " #   Column            Non-Null Count  Dtype         \n",
      "---  ------            --------------  -----         \n",
      " 0   cord_uid          7718 non-null   object        \n",
      " 1   source_x          7718 non-null   object        \n",
      " 2   title             7718 non-null   object        \n",
      " 3   doi               7677 non-null   object        \n",
      " 4   pmcid             4959 non-null   object        \n",
      " 5   pubmed_id         6233 non-null   object        \n",
      " 6   license           7718 non-null   object        \n",
      " 7   abstract          7718 non-null   object        \n",
      " 8   publish_time      7715 non-null   object        \n",
      " 9   authors           7674 non-null   object        \n",
      " 10  journal           6668 non-null   object        \n",
      " 11  mag_id            0 non-null      float64       \n",
      " 12  who_covidence_id  528 non-null    object        \n",
      " 13  arxiv_id          20 non-null     object        \n",
      " 14  label             7718 non-null   object        \n",
      " 15  time              7715 non-null   datetime64[ns]\n",
      " 16  timet             7718 non-null   int64         \n",
      "dtypes: datetime64[ns](1), float64(1), int64(1), object(14)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df_collection.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 608
    },
    "executionInfo": {
     "elapsed": 211,
     "status": "ok",
     "timestamp": 1742975978238,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "9veNFFGDZRx7",
    "outputId": "5eec7f85-7d20-44d7-8986-a85cb00533d8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>source_x</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>pmcid</th>\n",
       "      <th>pubmed_id</th>\n",
       "      <th>license</th>\n",
       "      <th>abstract</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal</th>\n",
       "      <th>mag_id</th>\n",
       "      <th>who_covidence_id</th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>label</th>\n",
       "      <th>time</th>\n",
       "      <th>timet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>umvrwgaw</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Professional and Home-Made Face Masks Reduce E...</td>\n",
       "      <td>10.1371/journal.pone.0002618</td>\n",
       "      <td>PMC2440799</td>\n",
       "      <td>18612429</td>\n",
       "      <td>cc-by</td>\n",
       "      <td>BACKGROUND: Governments are preparing for a po...</td>\n",
       "      <td>2008-07-09</td>\n",
       "      <td>van der Sande, Marianne; Teunis, Peter; Sabel,...</td>\n",
       "      <td>PLoS One</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>umvrwgaw</td>\n",
       "      <td>2008-07-09</td>\n",
       "      <td>1215561600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>spiud6ok</td>\n",
       "      <td>PMC</td>\n",
       "      <td>The Failure of R (0)</td>\n",
       "      <td>10.1155/2011/527610</td>\n",
       "      <td>PMC3157160</td>\n",
       "      <td>21860658</td>\n",
       "      <td>cc-by</td>\n",
       "      <td>The basic reproductive ratio, R (0), is one of...</td>\n",
       "      <td>2011-08-16</td>\n",
       "      <td>Li, Jing; Blakeley, Daniel; Smith?, Robert J.</td>\n",
       "      <td>Comput Math Methods Med</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>spiud6ok</td>\n",
       "      <td>2011-08-16</td>\n",
       "      <td>1313452800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>aclzp3iy</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Pulmonary sequelae in a patient recovered from...</td>\n",
       "      <td>10.4103/0970-2113.99118</td>\n",
       "      <td>PMC3424870</td>\n",
       "      <td>22919170</td>\n",
       "      <td>cc-by-nc-sa</td>\n",
       "      <td>The pandemic of swine flu (H1N1) influenza spr...</td>\n",
       "      <td>2012</td>\n",
       "      <td>Singh, Virendra; Sharma, Bharat Bhushan; Patel...</td>\n",
       "      <td>Lung India</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>aclzp3iy</td>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>1325376000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>ycxyn2a2</td>\n",
       "      <td>PMC</td>\n",
       "      <td>What was the primary mode of smallpox transmis...</td>\n",
       "      <td>10.3389/fcimb.2012.00150</td>\n",
       "      <td>PMC3509329</td>\n",
       "      <td>23226686</td>\n",
       "      <td>cc-by</td>\n",
       "      <td>The mode of infection transmission has profoun...</td>\n",
       "      <td>2012-11-29</td>\n",
       "      <td>Milton, Donald K.</td>\n",
       "      <td>Front Cell Infect Microbiol</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ycxyn2a2</td>\n",
       "      <td>2012-11-29</td>\n",
       "      <td>1354147200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>zxe95qy9</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Lessons from the History of Quarantine, from P...</td>\n",
       "      <td>10.3201/eid1902.120312</td>\n",
       "      <td>PMC3559034</td>\n",
       "      <td>23343512</td>\n",
       "      <td>no-cc</td>\n",
       "      <td>In the new millennium, the centuries-old strat...</td>\n",
       "      <td>2013-02-03</td>\n",
       "      <td>Tognotti, Eugenia</td>\n",
       "      <td>Emerg Infect Dis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>zxe95qy9</td>\n",
       "      <td>2013-02-03</td>\n",
       "      <td>1359849600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cord_uid source_x                                              title  \\\n",
       "162   umvrwgaw      PMC  Professional and Home-Made Face Masks Reduce E...   \n",
       "611   spiud6ok      PMC                               The Failure of R (0)   \n",
       "918   aclzp3iy      PMC  Pulmonary sequelae in a patient recovered from...   \n",
       "993   ycxyn2a2      PMC  What was the primary mode of smallpox transmis...   \n",
       "1053  zxe95qy9      PMC  Lessons from the History of Quarantine, from P...   \n",
       "\n",
       "                               doi       pmcid pubmed_id      license  \\\n",
       "162   10.1371/journal.pone.0002618  PMC2440799  18612429        cc-by   \n",
       "611            10.1155/2011/527610  PMC3157160  21860658        cc-by   \n",
       "918        10.4103/0970-2113.99118  PMC3424870  22919170  cc-by-nc-sa   \n",
       "993       10.3389/fcimb.2012.00150  PMC3509329  23226686        cc-by   \n",
       "1053        10.3201/eid1902.120312  PMC3559034  23343512        no-cc   \n",
       "\n",
       "                                               abstract publish_time  \\\n",
       "162   BACKGROUND: Governments are preparing for a po...   2008-07-09   \n",
       "611   The basic reproductive ratio, R (0), is one of...   2011-08-16   \n",
       "918   The pandemic of swine flu (H1N1) influenza spr...         2012   \n",
       "993   The mode of infection transmission has profoun...   2012-11-29   \n",
       "1053  In the new millennium, the centuries-old strat...   2013-02-03   \n",
       "\n",
       "                                                authors  \\\n",
       "162   van der Sande, Marianne; Teunis, Peter; Sabel,...   \n",
       "611       Li, Jing; Blakeley, Daniel; Smith?, Robert J.   \n",
       "918   Singh, Virendra; Sharma, Bharat Bhushan; Patel...   \n",
       "993                                   Milton, Donald K.   \n",
       "1053                                  Tognotti, Eugenia   \n",
       "\n",
       "                          journal  mag_id who_covidence_id arxiv_id     label  \\\n",
       "162                      PLoS One     NaN              NaN      NaN  umvrwgaw   \n",
       "611       Comput Math Methods Med     NaN              NaN      NaN  spiud6ok   \n",
       "918                    Lung India     NaN              NaN      NaN  aclzp3iy   \n",
       "993   Front Cell Infect Microbiol     NaN              NaN      NaN  ycxyn2a2   \n",
       "1053             Emerg Infect Dis     NaN              NaN      NaN  zxe95qy9   \n",
       "\n",
       "           time       timet  \n",
       "162  2008-07-09  1215561600  \n",
       "611  2011-08-16  1313452800  \n",
       "918  2012-01-01  1325376000  \n",
       "993  2012-11-29  1354147200  \n",
       "1053 2013-02-03  1359849600  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_collection.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAUiDU0xXLBt"
   },
   "source": [
    "## 1.b) Import the query set\n",
    "\n",
    "The query set contains tweets with implicit references to academic papers from the collection set.\n",
    "\n",
    "The preprocessed query set is available on the Gitlab repository here: https://gitlab.com/checkthat_lab/clef2025-checkthat-lab/-/tree/main/task4/subtask_4b\n",
    "\n",
    "Participants should first download the file then upload it on the Google Colab session with the following steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1742975982410,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "v8gwkZDSXPsd"
   },
   "outputs": [],
   "source": [
    "# 1) Download the query tweets from the Gitlab repository: https://gitlab.com/checkthat_lab/clef2025-checkthat-lab/-/tree/main/task4/subtask_4b?ref_type=heads\n",
    "# 2) Drag and drop the downloaded file to the \"Files\" section (left vertical menu on Colab)\n",
    "# 3) Modify the path to your local file path\n",
    "PATH_QUERY_TRAIN_DATA = 'subtask4b_query_tweets_train.tsv' #MODIFY PATH\n",
    "PATH_QUERY_DEV_DATA = 'subtask4b_query_tweets_dev.tsv' #MODIFY PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1742976006985,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "VqxjYq2tYDmE"
   },
   "outputs": [],
   "source": [
    "df_query_train = pd.read_csv(PATH_QUERY_TRAIN_DATA, sep = '\\t')\n",
    "df_query_dev = pd.read_csv(PATH_QUERY_DEV_DATA, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "szMEK3OkYLvX"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cord_uid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Oral care in rehabilitation medicine: oral vul...</td>\n",
       "      <td>htlvpvz5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>this study isn't receiving sufficient attentio...</td>\n",
       "      <td>4kfl29ul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>thanks, xi jinping. a reminder that this study...</td>\n",
       "      <td>jtwb17u8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Taiwan - a population of 23 million has had ju...</td>\n",
       "      <td>0w9k8iy1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Obtaining a diagnosis of autism in lower incom...</td>\n",
       "      <td>tiqksd69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id                                         tweet_text  cord_uid\n",
       "0        0  Oral care in rehabilitation medicine: oral vul...  htlvpvz5\n",
       "1        1  this study isn't receiving sufficient attentio...  4kfl29ul\n",
       "2        2  thanks, xi jinping. a reminder that this study...  jtwb17u8\n",
       "3        3  Taiwan - a population of 23 million has had ju...  0w9k8iy1\n",
       "4        4  Obtaining a diagnosis of autism in lower incom...  tiqksd69"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_query_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "aslmTTJQyL2X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12853 entries, 0 to 12852\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   post_id     12853 non-null  int64 \n",
      " 1   tweet_text  12853 non-null  object\n",
      " 2   cord_uid    12853 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 301.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_query_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1742976030778,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "B5X8FwLhLY3u",
    "outputId": "36e21737-8257-4568-8346-0d3e0980ee53"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cord_uid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>covid recovery: this study from the usa reveal...</td>\n",
       "      <td>3qvh482o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69</td>\n",
       "      <td>\"Among 139 clients exposed to two symptomatic ...</td>\n",
       "      <td>r58aohnu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>73</td>\n",
       "      <td>I recall early on reading that researchers who...</td>\n",
       "      <td>sts48u9i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>93</td>\n",
       "      <td>You know you're credible when NIH website has ...</td>\n",
       "      <td>3sr2exq9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>96</td>\n",
       "      <td>Resistance to antifungal medications is a grow...</td>\n",
       "      <td>ybwwmyqy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id                                         tweet_text  cord_uid\n",
       "0       16  covid recovery: this study from the usa reveal...  3qvh482o\n",
       "1       69  \"Among 139 clients exposed to two symptomatic ...  r58aohnu\n",
       "2       73  I recall early on reading that researchers who...  sts48u9i\n",
       "3       93  You know you're credible when NIH website has ...  3sr2exq9\n",
       "4       96  Resistance to antifungal medications is a grow...  ybwwmyqy"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_query_dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1742976032804,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "t6gDlBZnLcdH",
    "outputId": "11cd57d2-a4b7-4b06-a9af-9ba5e29c191b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1400 entries, 0 to 1399\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   post_id     1400 non-null   int64 \n",
      " 1   tweet_text  1400 non-null   object\n",
      " 2   cord_uid    1400 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 32.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df_query_dev.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jr_BDzufPmmP"
   },
   "source": [
    "# 2) Running the baseline\n",
    "The following code runs a BM25 baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3156,
     "status": "ok",
     "timestamp": 1742976045832,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "BHfJ7ItxO8u8",
    "outputId": "8a4d8f08-31c0-4a9d-814e-b921b80fbe35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_def.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_avx2.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_core.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_intel_lp64.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_intel_thread.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_def.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_avx2.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_core.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_intel_lp64.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_intel_thread.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "Requirement already satisfied: rank_bm25 in /opt/conda/lib/python3.11/site-packages (0.2.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from rank_bm25) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install rank_bm25\n",
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 1414,
     "status": "ok",
     "timestamp": 1742976047296,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "jXCC7K_ZPQL2"
   },
   "outputs": [],
   "source": [
    "# Create the BM25 corpus\n",
    "corpus = df_collection[:][['title', 'abstract']].apply(lambda x: f\"{x['title']} {x['abstract']}\", axis=1).tolist()\n",
    "cord_uids = df_collection[:]['cord_uid'].tolist()\n",
    "tokenized_corpus = [doc.split(' ') for doc in corpus]\n",
    "bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1742976047304,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "e8NeJWGYPQZG"
   },
   "outputs": [],
   "source": [
    "def get_top_cord_uids(query):\n",
    "  text2bm25top = {}\n",
    "  if query in text2bm25top.keys():\n",
    "      return text2bm25top[query]\n",
    "  else:\n",
    "      tokenized_query = query.split(' ')\n",
    "      doc_scores = bm25.get_scores(tokenized_query)\n",
    "      indices = np.argsort(-doc_scores)[:5]\n",
    "      bm25_topk = [cord_uids[x] for x in indices]\n",
    "\n",
    "      text2bm25top[query] = bm25_topk\n",
    "      return bm25_topk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fk4-BqEtTgUj"
   },
   "outputs": [],
   "source": [
    "# Retrieve topk candidates using the BM25 model\n",
    "df_query_train['bm25_topk'] = df_query_train['tweet_text'].apply(lambda x: get_top_cord_uids(x))\n",
    "df_query_dev['bm25_topk'] = df_query_dev['tweet_text'].apply(lambda x: get_top_cord_uids(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVKBlTCZUMSc"
   },
   "source": [
    "# 3) Evaluating the baseline\n",
    "The following code evaluates the BM25 retrieval baseline on the query set using the Mean Reciprocal Rank score (MRR@5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1742976555898,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "c-vdGWXXTgjZ"
   },
   "outputs": [],
   "source": [
    "# Evaluate retrieved candidates using MRR@k\n",
    "def get_performance_mrr(data, col_gold, col_pred, list_k = [1, 5, 10]):\n",
    "    d_performance = {}\n",
    "    for k in list_k:\n",
    "        data[\"in_topx\"] = data.apply(lambda x: (1/([i for i in x[col_pred][:k]].index(x[col_gold]) + 1) if x[col_gold] in [i for i in x[col_pred][:k]] else 0), axis=1)\n",
    "        # performances.append(data[\"in_topx\"].mean())\n",
    "        d_performance[k] = data[\"in_topx\"].mean()\n",
    "    return d_performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1742976568622,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "xLX9SMg5USkH",
    "outputId": "7c414679-6486-4e08-dffe-23d8cffbddf0"
   },
   "outputs": [],
   "source": [
    "results_train = get_performance_mrr(df_query_train, 'cord_uid', 'bm25_topk')\n",
    "results_dev = get_performance_mrr(df_query_dev, 'cord_uid', 'bm25_topk')\n",
    "# Printed MRR@k results in the following format: {k: MRR@k}\n",
    "print(f\"Results on the train set: {results_train}\")\n",
    "print(f\"Results on the dev set: {results_dev}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RazcRTV84KQC"
   },
   "source": [
    "# 4) Exporting results to prepare the submission on Codalab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1742976603546,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "DFng4ocDw3Hk"
   },
   "outputs": [],
   "source": [
    "df_query_dev['preds'] = df_query_dev['bm25_topk'].apply(lambda x: x[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1742976608184,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "nAVBQYh_xP8O"
   },
   "outputs": [],
   "source": [
    "df_query_dev[['post_id', 'preds']].to_csv('predictions.tsv', index=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "opGI1H1h4Og5"
   },
   "source": [
    "# Implementing Neural Re-ranking Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we are extending a BM25 retrieval baseline by implementing a Neural Re-ranking Model. The BM25 gives us a first-stage candidate list, and our neural model learns to rerank those candidates by jointly modeling the semantic relationship between the tweet and each paper (title + abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Installing & LoadingDependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_def.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_avx2.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_core.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_intel_lp64.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_intel_thread.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_def.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_avx2.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_core.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_intel_lp64.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_intel_thread.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n"
     ]
    }
   ],
   "source": [
    "!pip install -q torch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 10:56:58.881426: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-21 10:56:58.900401: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-21 10:56:58.906016: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-21 10:56:58.920984: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-21 10:56:59.922025: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-encoder ready for reranking!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "# Load the pretrained cross-encoder model (which is a lightweight BERT-based ranker)\n",
    "tokenizer=AutoTokenizer.from_pretrained(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "model=AutoModelForSequenceClassification.from_pretrained(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "# pipeline for pairwise scoring\n",
    "reranker = TextClassificationPipeline(model=model,tokenizer=tokenizer,return_all_scores=False,\n",
    "    device=-1 # set to 0 if GPU exists)\n",
    "print(\"Cross-encoder ready for reranking!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Loading Predictions and Collection, Prepare Rerank Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first read in the BM25 baseline predictions and the dev tweet queries, ensuring the join key post_id was of a uniform string type to avoid merge errors. We loaded the CORD-19 paper metadata (7,718 documents), concatenated each paper’s title and abstract into a single text field (paper_text), and built a lookup dictionary from cord_uid to paper_text.\n",
    "Next, we took a look at the top-5 predictions for each tweet into individual rows—resulting in N_tweets × 5 rows—each. we assembled a list of (tweet_text, paper_text) pairs, which will be fed to our cross-encoder reranker.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframe for reranking size: (7000, 6)\n",
      "total pairs: 7000\n"
     ]
    }
   ],
   "source": [
    "# Loading BM25 predictions\n",
    "df_preds = pd.read_csv(\"predictions.tsv\",sep=\"\\t\",converters={\"preds\": eval})\n",
    "\n",
    "df_preds[\"post_id\"] = df_preds[\"post_id\"].astype(str)\n",
    "\n",
    "# Loading dev queries\n",
    "df_dev = pd.read_csv(\"subtask4b_query_tweets_dev.tsv\",sep=\"\\t\",names=[\"post_id\", \"tweet_text\", \"cord_uid\"],)\n",
    "\n",
    "df_dev[\"post_id\"] = df_dev[\"post_id\"].astype(str)\n",
    "\n",
    "# Loading collection metadata\n",
    "with open(\"subtask4b_collection_data.pkl\", \"rb\") as f:\n",
    "    df_papers = pickle.load(f)\n",
    "\n",
    "df_papers[\"paper_text\"] = (\n",
    "    df_papers[\"title\"].fillna(\"\") + \" \" + df_papers[\"abstract\"].fillna(\"\"))\n",
    "paper_text_lookup = df_papers.set_index(\"cord_uid\")[\"paper_text\"].to_dict()\n",
    "\n",
    "rows = []\n",
    "for _, row in df_preds.iterrows():\n",
    "    post_id = row[\"post_id\"]\n",
    "    for rank, cand_uid in enumerate(row[\"preds\"], start=1):\n",
    "        rows.append({\"post_id\": post_id, \"rank\": rank, \"cand_uid\": cand_uid})\n",
    "df_rerank = pd.DataFrame(rows)\n",
    "\n",
    "# Merge tweet text and ground-truth cord_uid\n",
    "df_rerank = df_rerank.merge(df_dev[[\"post_id\", \"tweet_text\", \"cord_uid\"]],on=\"post_id\",how=\"left\")\n",
    "\n",
    "df_rerank[\"paper_text\"] = df_rerank[\"cand_uid\"].map(paper_text_lookup)\n",
    "\n",
    "pairs = list(zip(df_rerank[\"tweet_text\"].tolist(), df_rerank[\"paper_text\"].tolist()))\n",
    "\n",
    "print(f\"dataframe for reranking size: {df_rerank.shape}\")\n",
    "print(f\"total pairs: {len(pairs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Computing Cross-Encoder Scores in Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the HuggingFace cross-encoder/ms-marco-MiniLM-L-6-v2 model to jointly score each tweet–paper pair. To scale to our 5 × N_dev inputs without running out of memory, we processed pairs in mini-batches of 32. Each batch was tokenized as [CLS] tweet [SEP] paper_text [SEP] and passed through the model in evaluation mode. We extracted the scalar relevance score (for regression heads) or the probability/logit of the “relevant” class (for classification heads), and stored these in the DataFrame. These scores will drive our new reranking order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigned reranking scores. Sample:\n",
      "   post_id  rank  cand_uid                                         tweet_text  \\\n",
      "0      16     1  25aj8rj5  covid recovery: this study from the usa reveal...   \n",
      "1      16     2  gatxuwz7  covid recovery: this study from the usa reveal...   \n",
      "2      16     3  59up4v56  covid recovery: this study from the usa reveal...   \n",
      "3      16     4  styavbvi  covid recovery: this study from the usa reveal...   \n",
      "4      16     5  6sy80720  covid recovery: this study from the usa reveal...   \n",
      "\n",
      "   cord_uid                                         paper_text     score  \n",
      "0  3qvh482o  Long covid-mechanisms, risk factors, and manag...  0.787592  \n",
      "1  3qvh482o  Cerebral microvascular endothelial glycocalyx ... -2.689593  \n",
      "2  3qvh482o  Fatigue and Cognitive Impairment in Post-COVID...  4.022662  \n",
      "3  3qvh482o  COVCOG 2: Cognitive and Memory Deficits in Lon...  1.950408  \n",
      "4  3qvh482o  A further plot twist: will 'long COVID' have a... -5.372519  \n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "scores = []\n",
    "batch_size = 32\n",
    "\n",
    "for start in range(0, len(pairs), batch_size):\n",
    "    batch_pairs = pairs[start : start + batch_size]\n",
    "    queries = [q for (q, d) in batch_pairs]\n",
    "    docs    = [d for (q, d) in batch_pairs]\n",
    "    \n",
    "    # Tokenize the batch of pairs\n",
    "    encodings = tokenizer(queries, docs,padding=True,truncation=True,return_tensors=\"pt\")\n",
    "    device = next(model.parameters()).device\n",
    "    for k, v in encodings.items():\n",
    "        encodings[k] = v.to(device)\n",
    "\n",
    "    # Forward pass without gradients\n",
    "    with torch.no_grad():\n",
    "        logits = model(**encodings).logits\n",
    "\n",
    "    if logits.shape[-1] == 1:\n",
    "        batch_scores = logits.squeeze(-1).tolist()\n",
    "    else:\n",
    "        # 1: “relevant”\n",
    "        batch_scores = logits[:, 1].tolist()\n",
    "    scores.extend(batch_scores)\n",
    "\n",
    "df_rerank[\"score\"] = scores\n",
    "print(\"Assigned reranking scores. Sample:\\n\", df_rerank.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Re-rank and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Re-ranking results on dev set: {1: 0.535, 5: 0.5738809523809524, 10: 0.5738809523809524}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_273/3254219375.py:11: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: rerank_group(g))\n"
     ]
    }
   ],
   "source": [
    "# Build re-ranked predictions\n",
    "def rerank_group(group): # sort by the cross-encoder score and take top 5\n",
    "    top5 = group.sort_values(\"score\", ascending=False).head(5)\n",
    "    return list(top5[\"cand_uid\"])\n",
    "\n",
    "# Apply over each post_id. Include only columns: ['post_id','preds']\n",
    "df_new_preds = (df_rerank.groupby(\"post_id\", group_keys=False).apply(lambda g: rerank_group(g)).reset_index(name=\"preds\"))\n",
    "\n",
    "# Merge with ground-truth cord_uid for evaluation\n",
    "df_eval = df_new_preds.merge(df_dev[[\"post_id\", \"cord_uid\"]],on=\"post_id\",how=\"left\")\n",
    "\n",
    "# MRR@ 1,5,10\n",
    "results_rerank = get_performance_mrr(df_eval,col_gold=\"cord_uid\",col_pred=\"preds\",list_k=[1, 5, 10])\n",
    "print(f\"Neural Re-ranking results on dev set: {results_rerank}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integrating a BERT-based cross-encoder (cross-encoder/ms-marco-MiniLM-L-6-v2) to rerank the BM25 candidates yielded clear gains over the lexical baseline. On the dev set, MRR@1 improved from 0.505 to 0.535 (+3.0 pts), and MRR@5 improved from 0.552 to 0.574 (+2.2 pts). These improvements demonstrate that joint modeling of tweet and paper text captures semantic relevance signals that BM25 misses, allowing more correct papers to be promoted into the top ranks.\n",
    "\n",
    "MRR@1: how often the top re-ranked candidate is correct.\n",
    "\n",
    "MRR@5: the mean reciprocal rank among the top 5 (primary metric).\n",
    "\n",
    "MRR@10: extended view up to position 10.\n",
    "\n",
    "Compared to the BM25 baseline on dev ({1: 0.505, 5: 0.552, 10: 0.552}), the neural re-ranker achieves:\n",
    "\n",
    "MRR@1: ↑ from 0.505 → 0.535 (an absolute gain of +0.030)\n",
    "\n",
    "MRR@5: ↑ from 0.552 → 0.574 (an absolute gain of +0.022)\n",
    "\n",
    "MRR@10: same as MRR@5 here, also ↑ +0.022\n",
    "\n",
    "As observed, The cross-encoder reranker improves the ranking quality, especially pushing the correct paper to the very top more often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export in the required format: post_id \\t [\"pred1\", \"pred2\", ..., \"pred5\"]\n",
    "df_new_preds.to_csv(\"reranked_predictions.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Experiments:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1: Linear Score Fusion (α × BM25 + (1-α) × Neural, α=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first experiment, We combined BM25 and neural cross‐encoder scores by first normalizing each within every tweet (post_id), then taking:\n",
    "\n",
    "fusion_score = 0.5×bm25_norm + 0.5×neural_norm.\n",
    "\n",
    "We re‐ranked each tweet’s 5 candidates by this new fusion score and evaluated MRR@1, MRR@5, MRR@10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_273/926457635.py:9: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_rerank[\"bm25_norm\"]  = df_rerank.groupby(\"post_id\").apply(lambda g: normalize(g, \"bm25_score\")).reset_index(0,drop=True)\n",
      "/tmp/ipykernel_273/926457635.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_rerank[\"neural_norm\"] = df_rerank.groupby(\"post_id\").apply(lambda g: normalize(g, \"score\")).reset_index(0,drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Fusion (α=0.5) MRR: {1: 0.5385714285714286, 5: 0.5749285714285715, 10: 0.5749285714285715}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_273/926457635.py:23: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: get_top5_lin(g))\n"
     ]
    }
   ],
   "source": [
    "# Simulating a BM25 “score” from higher rank to lower rank\n",
    "df_rerank[\"bm25_score\"] = -df_rerank[\"rank\"]\n",
    "\n",
    "# Normalizing bm25 and neural score per post_id\n",
    "def normalize(group, col):\n",
    "    vals = group[col]\n",
    "    return (vals - vals.min()) / (vals.max() - vals.min() + 1e-8)\n",
    "\n",
    "df_rerank[\"bm25_norm\"]  = df_rerank.groupby(\"post_id\").apply(lambda g: normalize(g, \"bm25_score\")).reset_index(0,drop=True)\n",
    "df_rerank[\"neural_norm\"] = df_rerank.groupby(\"post_id\").apply(lambda g: normalize(g, \"score\")).reset_index(0,drop=True)\n",
    "\n",
    "# fusion scores\n",
    "alpha = 0.5\n",
    "df_rerank[\"fusion_score_lin\"] = alpha * df_rerank[\"bm25_norm\"] + (1-alpha) * df_rerank[\"neural_norm\"]\n",
    "\n",
    "# Re-ranking and collecting top-5\n",
    "def get_top5_lin(g):\n",
    "    return list(g.sort_values(\"fusion_score_lin\", ascending=False)[\"cand_uid\"].head(5))\n",
    "\n",
    "df_preds_fusion_lin = (df_rerank.groupby(\"post_id\", group_keys=False).apply(lambda g: get_top5_lin(g)).reset_index(name=\"preds\"))\n",
    "\n",
    "# Evaluation\n",
    "df_eval_lin = df_preds_fusion_lin.merge(df_dev[[\"post_id\",\"cord_uid\"]],on=\"post_id\", how=\"left\")\n",
    "results_fusion_lin = get_performance_mrr(df_eval_lin, \"cord_uid\", \"preds\", [1,5,10])\n",
    "print(\"Linear Fusion (α=0.5) MRR:\", results_fusion_lin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison to previous models:\n",
    "\n",
    "Model:                   MRR@1   MRR@5   MRR@10\n",
    "\n",
    "BM25 baseline:\t         0.505   0.552   0.552\n",
    "\n",
    "Neural Re-ranking:\t     0.535   0.574   0.574\n",
    "\n",
    "Linear Fusion (α=0.5):\t 0.539   0.575   0.575\n",
    "\n",
    "MRR@1 improved from:     0.535 → 0.539 (+0.004)\n",
    "\n",
    "MRR@5 improved from:     0.574 → 0.575 (+0.001)\n",
    "\n",
    "Based on the results, The simple 50/50 fusion of lexical (BM25) and semantic (neural) scores yields a small but measurable gain over the pure neural reranker. This suggests that BM25 still contributes complementary signals—especially in borderline cases where the cross‐encoder might under‐ or over‐score certain candidates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment b (Reciprocal-Rank Fusion Results & Interpretation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second experiment, We compute each candidate’s rank under both BM25 and the neural cross-encoder. We then convert these to reciprocal ranks (1/r) and sum them to get a fused score.\n",
    "Rank-based fusion is robust to differing score scales and emphasizes candidates that score well under both methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reciprocal-Rank Fusion MRR: {1: 0.5278571428571428, 5: 0.5709761904761904, 10: 0.5709761904761904}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_273/2643861837.py:22: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: get_top5_rr(g))\n"
     ]
    }
   ],
   "source": [
    "# Computing neural rank per post_id\n",
    "df_rerank[\"neural_rank\"] = (df_rerank.groupby(\"post_id\")[\"score\"].rank(ascending=False, method=\"first\"))\n",
    "\n",
    "# Compute reciprocal ranks for BM25 and neural\n",
    "df_rerank[\"rr_bm25\"]=1.0/df_rerank[\"rank\"]\n",
    "df_rerank[\"rr_neural\"]=1.0/df_rerank[\"neural_rank\"]\n",
    "\n",
    "# Sum reciprocal ranks for fusion score\n",
    "df_rerank[\"fusion_score_rr\"]=df_rerank[\"rr_bm25\"]+df_rerank[\"rr_neural\"]\n",
    "\n",
    "# Re-rank by the sum of reciprocal-rank\n",
    "def get_top5_rr(group):\n",
    "    return list(group.sort_values(\"fusion_score_rr\", ascending=False)[\"cand_uid\"].head(5))\n",
    "\n",
    "df_preds_fusion_rr = (df_rerank.groupby(\"post_id\", group_keys=False)\n",
    "                      .apply(lambda g: get_top5_rr(g)).reset_index(name=\"preds\"))\n",
    "\n",
    "df_eval_rr = df_preds_fusion_rr.merge(df_dev[[\"post_id\", \"cord_uid\"]],on=\"post_id\",how=\"left\")\n",
    "\n",
    "# Evaluation with MRR@ 1,5,10\n",
    "results_fusion_rr = get_performance_mrr(df_eval_rr, \"cord_uid\", \"preds\", [1, 5, 10])\n",
    "print(\"Reciprocal-Rank Fusion MRR:\", results_fusion_rr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison:\n",
    "\n",
    "Model:\tMRR@1,MRR@5\n",
    "\n",
    "BM25 baseline:\t0.505,0.552\n",
    "\n",
    "Neural Re-ranking:\t0.535,0.574\n",
    "\n",
    "Linear Score Fusion: (α=0.5) 0.539,0.575\n",
    "\n",
    "Reciprocal-Rank Fusion: 0.528,0.571\n",
    "\n",
    "Reciprocal-Rank Fusion underperforms compared to pure neural and linear fusion:\n",
    "\n",
    "MRR@1 falls from 0.535 → 0.528 (−0.007)\n",
    "\n",
    "MRR@5 falls from 0.574 → 0.571 (−0.003)\n",
    "\n",
    "Comparing the results we can conclude that, Summing reciprocal ranks gives too much weight to small rank differences and is less effective here. Linear fusion proved the most robust at balancing BM25’s lexical signals with the cross-encoder’s semantic scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is Experiment 3) (a) simple α‐weight tuning for the linear score fusion. We’ll change α value from 0.0 to 1.0 (with step 0.1), compute MRR@5 for each, and pick the best.\n",
    "For Evaluation, we re‐rank the same top‐5 candidates and compute MRR@5 For each α. this way we can find the best α that maximizes dev MRR@5, this can show whether a different balance beats the 0.5 default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_273/1868486211.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_rerank[\"bm25_norm\"]   = df_rerank.groupby(\"post_id\").apply(lambda g: normalize(g, \"bm25_score\")).reset_index(0,drop=True)\n",
      "/tmp/ipykernel_273/1868486211.py:11: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_rerank[\"neural_norm\"] = df_rerank.groupby(\"post_id\").apply(lambda g: normalize(g, \"score\")).reset_index(0,drop=True)\n",
      "/tmp/ipykernel_273/1868486211.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: list(g.sort_values(\"fusion_score\", ascending=False)[\"cand_uid\"].head(5)))\n",
      "/tmp/ipykernel_273/1868486211.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: list(g.sort_values(\"fusion_score\", ascending=False)[\"cand_uid\"].head(5)))\n",
      "/tmp/ipykernel_273/1868486211.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: list(g.sort_values(\"fusion_score\", ascending=False)[\"cand_uid\"].head(5)))\n",
      "/tmp/ipykernel_273/1868486211.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: list(g.sort_values(\"fusion_score\", ascending=False)[\"cand_uid\"].head(5)))\n",
      "/tmp/ipykernel_273/1868486211.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: list(g.sort_values(\"fusion_score\", ascending=False)[\"cand_uid\"].head(5)))\n",
      "/tmp/ipykernel_273/1868486211.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: list(g.sort_values(\"fusion_score\", ascending=False)[\"cand_uid\"].head(5)))\n",
      "/tmp/ipykernel_273/1868486211.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: list(g.sort_values(\"fusion_score\", ascending=False)[\"cand_uid\"].head(5)))\n",
      "/tmp/ipykernel_273/1868486211.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: list(g.sort_values(\"fusion_score\", ascending=False)[\"cand_uid\"].head(5)))\n",
      "/tmp/ipykernel_273/1868486211.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: list(g.sort_values(\"fusion_score\", ascending=False)[\"cand_uid\"].head(5)))\n",
      "/tmp/ipykernel_273/1868486211.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: list(g.sort_values(\"fusion_score\", ascending=False)[\"cand_uid\"].head(5)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "α vs. MRR@5:\n",
      " α=0.0 → MRR@5=0.5739\n",
      " α=0.1 → MRR@5=0.5760\n",
      " α=0.2 → MRR@5=0.5776\n",
      " α=0.3 → MRR@5=0.5754\n",
      " α=0.4 → MRR@5=0.5739\n",
      " α=0.5 → MRR@5=0.5749\n",
      " α=0.6 → MRR@5=0.5696\n",
      " α=0.7 → MRR@5=0.5637\n",
      " α=0.8 → MRR@5=0.5555\n",
      " α=0.9 → MRR@5=0.5520\n",
      " α=1.0 → MRR@5=0.5520\n",
      "\n",
      "Best α=0.2 with MRR@5=0.5776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_273/1868486211.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: list(g.sort_values(\"fusion_score\", ascending=False)[\"cand_uid\"].head(5)))\n"
     ]
    }
   ],
   "source": [
    "# Introducing alphas:\n",
    "alphas=[i/10 for i in range(0,11)]\n",
    "results_alpha=[]\n",
    "\n",
    "df_rerank[\"bm25_score\"]=-df_rerank[\"rank\"]\n",
    "def normalize(group, col):\n",
    "    vals = group[col]\n",
    "    return (vals-vals.min())/(vals.max()-vals.min()+1e-8)\n",
    "    \n",
    "df_rerank[\"bm25_norm\"]=df_rerank.groupby(\"post_id\").apply(lambda g: normalize(g, \"bm25_score\")).reset_index(0,drop=True)\n",
    "df_rerank[\"neural_norm\"]=df_rerank.groupby(\"post_id\").apply(lambda g: normalize(g, \"score\")).reset_index(0,drop=True)\n",
    "\n",
    "for α in alphas:\n",
    "    df_rerank[\"fusion_score\"]=α*df_rerank[\"bm25_norm\"]+(1-α)*df_rerank[\"neural_norm\"]\n",
    "    # Re-Ranking\n",
    "    df_preds=(df_rerank.groupby(\"post_id\", group_keys=False).apply(lambda g: list(g.sort_values(\"fusion_score\", ascending=False)[\"cand_uid\"]\n",
    "                                                                                  .head(5))).reset_index(name=\"preds\"))\n",
    "    # Evaluation\n",
    "    df_eval = df_preds.merge(df_dev[[\"post_id\",\"cord_uid\"]],on=\"post_id\",how=\"left\")\n",
    "    perf = get_performance_mrr(df_eval,\"cord_uid\",\"preds\",[5])\n",
    "    results_alpha.append((α, perf[5]))\n",
    "\n",
    "print(\"α vs. MRR@5:\")\n",
    "for α, mrr5 in results_alpha:\n",
    "    print(f\" α={α:.1f} → MRR@5={mrr5:.4f}\")\n",
    "\n",
    "best_alpha, best_mrr5 = max(results_alpha, key=lambda x: x[1])\n",
    "print(f\"\\nBest α={best_alpha:.1f} with MRR@5={best_mrr5:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the results we can observe:\n",
    "for α = 0.0 (pure neural cross-encoder) we got MRR@5 = 0.5739  \n",
    "for α = 0.2 we got MRR@5 = 0.5776 (+0.0037 over pure neural, +0.0256 over BM25) which was the best mix. and for α=1.0 (or pure BM25) we got MRR@5 = 0.5520\n",
    " \n",
    "The optimal α=0.2 indicates that a heavier weighting on the neural model combined with a light lexical signal yields the best retrieval performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best on the whole experience on neural rerank:\n",
    "**Pure neural re-ranking** had the highest improvement to BM25 (+0.022 MRR@5).  \n",
    "**Linear fusion** added further gains; equal weighting is good, but best at α=0.2 (80% neural, 20% BM25) for maximum MRR@5.  \n",
    "**Reciprocal-rank fusion** was less effective, which demonstrated the importance of calibrated score fusion.  \n",
    "The best Experiment used α = 0.2, achieving **MRR@5 = 0.5776** on dev set."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
